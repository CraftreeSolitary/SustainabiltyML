{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3952946,"sourceType":"datasetVersion","datasetId":1554380}],"dockerImageVersionId":30665,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# competition_name = \"iamsouravbanerjee/animal-image-dataset-90-different-animals\"\n\n# kaggle_creds_path = \"./kaggle.json\"\n\n# ! pip install kaggle --quiet\n\n\n# ! mkdir ~/.kaggle\n# ! cp /content/kaggle.json ~/.kaggle/\n# ! chmod 600 ~/.kaggle/kagg|le.json\n\n# ! kaggle datasets download -d {competition_name}\n\n# ! mkdir kaggle_data","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZoG3K4q2MVkK","outputId":"04c04e8c-ea37-4d7c-9ec2-c2e699d72bed"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":"mkdir: cannot create directory ‘/root/.kaggle’: File exists\n\n/bin/bash: line 1: le.json: command not found\n\nchmod: cannot access '/root/.kaggle/kagg': No such file or directory\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n\nDownloading animal-image-dataset-90-different-animals.zip to /content\n\n 99% 650M/656M [00:04<00:00, 222MB/s]\n\n100% 656M/656M [00:04<00:00, 142MB/s]\n\nmkdir: cannot create directory ‘kaggle_data’: File exists\n"}]},{"cell_type":"code","source":"# ! unzip animal-image-dataset-90-different-animals.zip -d kaggle_data","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TTA98PC-M5Ry","outputId":"5cca2651-18e4-4f6e-be2e-e7c3297147f9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\n\noriginal_parent_dir = \"/kaggle/input/animal-image-dataset-90-different-animals/animals/animals\"\nnew_parent_dir = \"./onevrest\"\ntarget_folder = \"f2\"\nspecial_folder = \"badger\"\n\nos.makedirs(new_parent_dir, exist_ok=True)\n\n# Copy the special folder as-is\nshutil.copytree(os.path.join(original_parent_dir, special_folder),\n                os.path.join(new_parent_dir, special_folder))\n\n# Process the other folders efficiently\ntarget_dir = os.path.join(new_parent_dir, target_folder)\nos.makedirs(target_dir, exist_ok=True)\n\nfor dir in os.listdir(original_parent_dir):\n    dir_path = os.path.join(original_parent_dir, dir)\n    if os.path.isdir(dir_path) and dir != special_folder:\n        for file in os.listdir(dir_path):\n            file_src = os.path.join(dir_path, file)\n            file_dest = os.path.join(target_dir, file)\n            shutil.copy2(file_src, file_dest)\n\n        # Do NOT remove the source directories\n","metadata":{"id":"dpUcLcDHM6x6","execution":{"iopub.status.busy":"2024-03-09T23:35:22.131356Z","iopub.execute_input":"2024-03-09T23:35:22.132039Z","iopub.status.idle":"2024-03-09T23:36:06.715177Z","shell.execute_reply.started":"2024-03-09T23:35:22.132004Z","shell.execute_reply":"2024-03-09T23:36:06.714364Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\n\n# Original parent directory containing the 90 folders\noriginal_parent_dir = \"/kaggle/input/animal-image-dataset-90-different-animals/animals/animals\"\n\n# New parent directory where the 5 folders (d1 to d5) will be created\nnew_parent_dir = \"./fiveclass\"\n\n# Make sure the new parent directory exists\nos.makedirs(new_parent_dir, exist_ok=True)\n\n# Get a sorted list of all folders in the original parent directory\n# Assuming folder names allow them to be correctly sorted to reflect the desired order\nfolders = sorted([f for f in os.listdir(original_parent_dir) if os.path.isdir(os.path.join(original_parent_dir, f))])\n\n# Calculate the number of folders to distribute into each of the 5 new folders\nfolders_per_group = len(folders) // 5\n\n# Function to copy files from source to destination\ndef copy_files(src_dir, dest_dir):\n    os.makedirs(dest_dir, exist_ok=True)\n    for file in os.listdir(src_dir):\n        shutil.copy2(os.path.join(src_dir, file), os.path.join(dest_dir, file))\n\n# Iterate through each of the original folders and copy its contents to the correct new folder\nfor i, folder in enumerate(folders):\n    # Determine the index for the new folder (d1 to d5)\n    new_folder_index = i // folders_per_group + 1\n    if new_folder_index > 5:  # Ensure we don't go beyond d5\n        new_folder_index = 5\n\n    # Construct the new folder name and path\n    new_folder_name = f\"d{new_folder_index}\"\n    new_folder_path = os.path.join(new_parent_dir, new_folder_name)\n\n    # Copy the contents of the current folder to the new folder\n    current_folder_path = os.path.join(original_parent_dir, folder)\n    copy_files(current_folder_path, new_folder_path)\n\nprint(\"All folders have been processed and images distributed into d1 to d5.\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nLynvkduM9gZ","outputId":"f68ba9b2-c714-41a3-a62d-b70db51a4a40","execution":{"iopub.status.busy":"2024-03-09T22:28:52.934589Z","iopub.execute_input":"2024-03-09T22:28:52.934880Z","iopub.status.idle":"2024-03-09T22:29:05.093470Z","shell.execute_reply.started":"2024-03-09T22:28:52.934855Z","shell.execute_reply":"2024-03-09T22:29:05.092464Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"All folders have been processed and images distributed into d1 to d5.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision import datasets, models\nfrom torch.utils.data import DataLoader, random_split\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"id":"2uH-B6rcM_Wa","execution":{"iopub.status.busy":"2024-03-09T23:36:28.805165Z","iopub.execute_input":"2024-03-09T23:36:28.806004Z","iopub.status.idle":"2024-03-09T23:36:28.810631Z","shell.execute_reply.started":"2024-03-09T23:36:28.805968Z","shell.execute_reply":"2024-03-09T23:36:28.809703Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Transformations\ntransform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Load dataset\ndata_dir = 'onevrest'\ndata_dir2='fiveclass'\n","metadata":{"id":"NW5YLvKxNAw5","execution":{"iopub.status.busy":"2024-03-09T23:36:26.562931Z","iopub.execute_input":"2024-03-09T23:36:26.563723Z","iopub.status.idle":"2024-03-09T23:36:26.569125Z","shell.execute_reply.started":"2024-03-09T23:36:26.563687Z","shell.execute_reply":"2024-03-09T23:36:26.568256Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nkfold = KFold(n_splits=3, shuffle=True, random_state=42)","metadata":{"id":"EBJ8SkqaNmz5","execution":{"iopub.status.busy":"2024-03-09T22:29:10.860034Z","iopub.execute_input":"2024-03-09T22:29:10.860418Z","iopub.status.idle":"2024-03-09T22:29:11.441697Z","shell.execute_reply.started":"2024-03-09T22:29:10.860386Z","shell.execute_reply":"2024-03-09T22:29:11.440926Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import models\nfrom torch import nn, optim\nimport torch\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\nnum_classes = 2  # Adjust this based on your dataset\nnum_epochs = 2\nbatch_size = 32\n\nn_splits = 3  # Number of folds\n\nfull_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\nkfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nall_acc = []\nall_conf_matrices = []\n\nfor fold, (train_idx, test_idx) in enumerate(kfold.split(full_dataset)):\n    # Splitting datasets per fold\n    train_subset = Subset(full_dataset, train_idx)\n    test_subset = Subset(full_dataset, test_idx)\n\n    # Data loaders for the current fold\n    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n\n    # Reinitialize model and optimizer\n    model = models.convnext_tiny(weights=None)\n    num_ftrs = model.classifier[2].in_features\n    model.classifier[2] = nn.Linear(num_ftrs, num_classes)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    # Training loop for current fold\n    for epoch in range(num_epochs):\n        model.train()\n        for i, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n\n    # Evaluation for current fold\n    model.eval()\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n\n    acc = (np.array(y_true) == np.array(y_pred)).mean()\n    all_acc.append(acc)\n    conf_matrix = confusion_matrix(y_true, y_pred)\n    all_conf_matrices.append(conf_matrix)\n    print(f'Fold {fold+1}, Test Accuracy: {acc:.4f}')\n\n# Average accuracy and confusion matrix\naverage_acc = np.mean(all_acc)\naverage_conf_matrix = np.mean(all_conf_matrices, axis=0)\nprint(f'Average Test Accuracy: {average_acc:.4f}')\nprint('Average Confusion Matrix:')\nprint(average_conf_matrix)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"FWUo6P4VN_gv","outputId":"3dbf0bc9-ace7-421b-9ae5-61404064880f","execution":{"iopub.status.busy":"2024-03-09T22:31:56.335248Z","iopub.execute_input":"2024-03-09T22:31:56.335935Z","iopub.status.idle":"2024-03-09T22:41:46.121679Z","shell.execute_reply.started":"2024-03-09T22:31:56.335902Z","shell.execute_reply":"2024-03-09T22:41:46.120671Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Fold 1, Test Accuracy: 0.9900\nFold 2, Test Accuracy: 0.9883\nFold 3, Test Accuracy: 0.9883\nAverage Test Accuracy: 0.9889\nAverage Confusion Matrix:\n[[   0.           20.        ]\n [   0.         1779.66666667]]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import models\nfrom torch import nn, optim\nimport torch\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\nnum_classes = 2  # Adjust this based on your dataset\nnum_epochs = 2\nbatch_size = 32\n\nn_splits = 3  # Number of folds\n\nfull_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\nkfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nall_acc = []\nall_conf_matrices = []\n\nfor fold, (train_idx, test_idx) in enumerate(kfold.split(full_dataset)):\n    # Splitting datasets per fold\n    train_subset = Subset(full_dataset, train_idx)\n    test_subset = Subset(full_dataset, test_idx)\n\n    # Data loaders for the current fold\n    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n\n    # Reinitialize model and optimizer\n    model = models.resnet18(pretrained=False)\n    num_ftrs = model.fc.in_features\n    model.fc = nn.Linear(num_ftrs, num_classes)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    # Training loop for current fold\n    for epoch in range(num_epochs):\n        model.train()\n        for i, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n\n    # Evaluation for current fold\n    model.eval()\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n\n    acc = (np.array(y_true) == np.array(y_pred)).mean()\n    all_acc.append(acc)\n    conf_matrix = confusion_matrix(y_true, y_pred)\n    all_conf_matrices.append(conf_matrix)\n    print(f'Fold {fold+1}, Test Accuracy: {acc:.4f}')\n\n# Average accuracy and confusion matrix\naverage_acc = np.mean(all_acc)\naverage_conf_matrix = np.mean(all_conf_matrices, axis=0)\nprint(f'Average Test Accuracy: {average_acc:.4f}')\nprint('Average Confusion Matrix:')\nprint(average_conf_matrix)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":498},"id":"Glvt7xnyQ3RS","outputId":"df2255cd-4bef-4029-9ada-c578ae35e7e3","execution":{"iopub.status.busy":"2024-03-09T22:31:47.219342Z","iopub.status.idle":"2024-03-09T22:31:47.219754Z","shell.execute_reply.started":"2024-03-09T22:31:47.219540Z","shell.execute_reply":"2024-03-09T22:31:47.219557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import models\nfrom torch import nn, optim\nimport torch\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\nnum_classes = 5  # Adjust this based on your dataset\nnum_epochs = 3\nbatch_size = 32\n\nn_splits = 3  # Number of folds\n\nfull_dataset = datasets.ImageFolder(root=data_dir2, transform=transform)\nprint(full_dataset)\n\nkfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nall_acc = []\nall_conf_matrices = []\n\nfor fold, (train_idx, test_idx) in enumerate(kfold.split(full_dataset)):\n    # Splitting datasets per fold\n    train_subset = Subset(full_dataset, train_idx)\n    test_subset = Subset(full_dataset, test_idx)\n\n    # Data loaders for the current fold\n    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n\n    # Reinitialize model and optimizer\n    model = models.convnext_tiny(weights='IMAGENET1K_V1')\n    num_ftrs = model.classifier[2].in_features\n    model.classifier[2] = nn.Linear(num_ftrs, num_classes)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    # Training loop for current fold\n    for epoch in range(num_epochs):\n        model.train()\n        for i, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n\n    # Evaluation for current fold\n    model.eval()\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n\n    acc = (np.array(y_true) == np.array(y_pred)).mean()\n    all_acc.append(acc)\n    conf_matrix = confusion_matrix(y_true, y_pred)\n    all_conf_matrices.append(conf_matrix)\n    print(f'Fold {fold+1}, Test Accuracy: {acc:.4f}')\n\n# Average accuracy and confusion matrix\naverage_acc = np.mean(all_acc)\naverage_conf_matrix = np.mean(all_conf_matrices, axis=0)\nprint(f'Average Test Accuracy: {average_acc:.4f}')\nprint('Average Confusion Matrix:')\nprint(average_conf_matrix)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":671},"id":"gyGnP19ZSSSx","outputId":"e6a9d7c0-c63c-4324-ae49-20b5703da177","execution":{"iopub.status.busy":"2024-03-09T23:23:46.391558Z","iopub.execute_input":"2024-03-09T23:23:46.392217Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Dataset ImageFolder\n    Number of datapoints: 5400\n    Root location: fiveclass\n    StandardTransform\nTransform: Compose(\n               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=warn)\n               RandomHorizontalFlip(p=0.5)\n               ToTensor()\n               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n           )\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n100%|██████████| 109M/109M [00:07<00:00, 16.1MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Fold 1, Test Accuracy: 0.1950\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ComplexCustomCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ComplexCustomCNN, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n        self.bn4 = nn.BatchNorm2d(512)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.fc1 = nn.Linear(512 * 28 * 28, 1024)  # Adjusted for 224x224 input images\n        self.bn_fc1 = nn.BatchNorm1d(1024)\n        self.fc2 = nn.Linear(1024, 512)\n        self.bn_fc2 = nn.BatchNorm1d(512)\n        self.fc3 = nn.Linear(512, num_classes)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n        x = x.view(-1, 512 * 28 * 28)  # Adjusted for 224x224 input images\n        x = self.dropout(F.relu(self.bn_fc1(self.fc1(x))))\n        x = self.dropout(F.relu(self.bn_fc2(self.fc2(x))))\n        x = self.fc3(x)\n        return x\n\n# Example usage\nnum_classes = 10  # Adjust based on your dataset\nmodel = ComplexCustomCNN(num_classes=num_classes)\nprint(model)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-09T23:36:06.716670Z","iopub.execute_input":"2024-03-09T23:36:06.716962Z","iopub.status.idle":"2024-03-09T23:36:10.757364Z","shell.execute_reply.started":"2024-03-09T23:36:06.716936Z","shell.execute_reply":"2024-03-09T23:36:10.756519Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"ComplexCustomCNN(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=401408, out_features=1024, bias=True)\n  (bn_fc1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n  (bn_fc2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc3): Linear(in_features=512, out_features=10, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms\nfrom torch import nn, optim\nimport torch\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n\n\n# Define the transformation for the dataset\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nnum_epochs = 10\nbatch_size = 32\nn_splits = 3\nnum_classes = 5\n\nfull_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\nkfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nall_acc = []\nall_conf_matrices = []\n\nfor fold, (train_idx, test_idx) in enumerate(kfold.split(full_dataset)):\n    train_subset = Subset(full_dataset, train_idx)\n    test_subset = Subset(full_dataset, test_idx)\n\n    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n\n    model = ComplexCustomCNN(num_classes=num_classes)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    for epoch in range(num_epochs):\n        model.train()\n        for i, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n\n    model.eval()\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n\n    acc = (np.array(y_true) == np.array(y_pred)).mean()\n    all_acc.append(acc)\n    conf_matrix = confusion_matrix(y_true, y_pred)\n    all_conf_matrices.append(conf_matrix)\n    print(f'Fold {fold+1}, Test Accuracy: {acc:.4f}')\n\naverage_acc = np.mean(all_acc)\naverage_conf_matrix = np.mean(all_conf_matrices, axis=0)\nprint(f'Average Test Accuracy: {average_acc:.4f}')\nprint('Average Confusion Matrix:')\nprint(average_conf_matrix)\n","metadata":{"id":"CBnw1a-CUd7l","execution":{"iopub.status.busy":"2024-03-09T23:37:01.876874Z","iopub.execute_input":"2024-03-09T23:37:01.877590Z","iopub.status.idle":"2024-03-09T23:37:08.411232Z","shell.execute_reply.started":"2024-03-09T23:37:01.877559Z","shell.execute_reply":"2024-03-09T23:37:08.410004Z"},"trusted":true},"execution_count":9,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     48\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m---> 49\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mValueError\u001b[0m: Expected input batch_size (8) to match target batch_size (32)."],"ename":"ValueError","evalue":"Expected input batch_size (8) to match target batch_size (32).","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}